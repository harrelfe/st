---
title: "Bootstrap Confidence Limits for Bootstrap Overfitting-Corrected Model Performance"
author:
  - name: Frank Harrell 
    url: https://hbiostat.org
    affiliation: Department of Biostatistics<br>Vanderbilt University School of Medicine
date: 2025-07-24
date-modified: last-modified
categories: [computing, prediction, r, regression, validation, "2025"]
description: "The Efron-Gong optimism bootstrap has been used for decades to obtain reliable estimates of likely performance of statistical models on new data.  It accomplishes this by estimating the bias (optimism) from overfitting and subtracting that bias from apparent model performance indexes.  No fast reliable method for computing confidence intervals for overfitting-corrected measures currently exists, so analysts may have false confidence in internal model validations, especially for small datasets.  The purpose of this research is to empirically derive a satisfactory fast algorithm for computing the needed confidence intervals when the model is a a binary logistic regression model.  The approach is expected to work for a wide variety of models."
bibliography:
  - grateful-refs.bib
code-fold: false
draft: false
---

# Background

The goal here is strong internal validation after fitting a pre-specified regression model or one that was derived using backwards step-down variable selection such that the same variable selection procedure can be repeated afresh for each bootstrap repetition.  So strong internal validation means estimating a variety of model performance measures in a way that does not reward them for overfitting and that penalizes for all aspects of model selection and derivation that utilized the outcome variable.  The goal is estimating the likely future performance of the model on new data from the same stream as the data used in model development.  A standard approach, as implemented in the R `rms` package's `validate` and `calibrate` functions, is the Efron-Gong optimism bootstrap discussed in detail in [RMS](https://hbiostat.org/rmsc/validate).

A key omission from the `rms` package has been computation of uncertainty intervals for bootstrap overfitting-corrected measures.
In 2021 Hisashi Noma _et al._ wrote a very useful paper entitled [Confidence intervals of prediction accuracy measures for multivariable prediction models based on the bootstrap-based optimism correction methods](https://doi.org/10.1002/sim.9148).  They studied confidence interval coverage (although only in a two-tailed combined sense) of a full two-stage double bootstrap procedure to compute confidence intervals for Efron-Gong overfitting (bias/optimism)-corrected model performance measures.  This is a very computationally demanding but accurate procedure.  They also studied an approximate one-stage procedure that does not add to the computational burden of computing the overfitting-adjusted measures themselves.

Recall that the form of the bootstrap optimism-corrected measure $\tau$ for a single index such as a Brier score or rank correlation between predicted and observed outcomes is

$$\tau = \theta + \bar{\theta}_{b} - \bar{\theta}_{w}$$

where $\theta$ is the original index computed as usual on the whole sample (sometimes called the _apparent_ index), $\theta_b$ is the apparent performance in a bootstrap sample when model coefficients are fitted in the bootstrap sample, $\theta_w$ is the performance of the bootstrap-fitted model on the original whole sample, and the horizontal bars represent averages over $B$ bootstrap resamples.  The estimated bias is $\gamma = \bar{\theta}_{b} - \bar{\theta}_{w}$.

Noma _et al._'s approximate bootstrap confidence intervals take the bootstrap distribution of $\theta_{b}$, computes confidence intervals from it using the bootstrap percentile nonparametric method, and shifts the resulting intervals by $\gamma$.  They showed that in large samples, taking into account only the variation in $\bar{\theta}_{b}$ is good enough, and that the overall confidence interval coverage was close to that of the expensive double bootstrap.  But when the sample size is not large, considering only variation in $\theta_b$ significantly underestimates uncertainties, and confidence interval coverage is far below the intended level (of 0.95, for example).

# Fast Alternative Bootstrap Confidence Limits

To derive a fast approximate method that I expect to have better confidence coverage for non-large $n$, consider $B$ individual bootstrap sample estimates $\theta_{b}$ and  $\theta_{w}$ and compute variances of several combinations of them, including $\text{var}(\theta_{b})$, $\text{var}(\theta_{w})$, $\text{var}(\gamma)$, variance of weighted sums of $\theta_{b}$ and  $\theta_{w}$, and other variants.
Variances of vectors involving both $\theta_{b}$ and  $\theta_{w}$ takes into account uncertainties in two quantities instead of one.  One can think of the variance of a difference as a summative measure for two uncertainties, because the variance of a difference is the sum of the variances minus twice the covariance of the two.  It is tempting to use the bootstrap nonparametric percentile interval, but we need to respect the width of the bootstrap distribution for the combination of $\theta_{b}$ and  $\theta_{w}$ and pay no attention to its center.  This pushes us to use normal-approximation bootstrap confidence intervals, which take an original point estimate (here a bootstrap bias-corrected index) $\pm 1.96\times$ the bootstrap standard deviation, for example.

An additional complexity is that the bootstrap distribution may not be symmetric, and assuming that it is (as the standard deviation does) leads to symmetric confidence intervals instead of the needed asymmetric ones.  The result is inaccurate confidence coverage in at least one of the tails.  To fix that problem, we compute two standard deviations: the SD for upper values and the SD for lower ones.  There is a question of where to make the high/low split.  Traditionally the sample mean is chosen.  Each of the two confidence limits is computed using the square root of the average squared distance from the overall mean of the bootstrap distribution.

Next we attempt to find the best combination of variation of training and test indexes from bootstrap repetitions so that confidence interval coverage for overfitted-corrected indexes is good in both tails for three performance measures. 
The resulting bootstrap method will be called ABCLOC for **a**symmetric **b**ootstrap **c**onfidence **l**imits for **o**verfitting-**c**orrected model performance metrics.

# Simulation to Derive ABCLOC to Obtain Accurate Confidence Coverage for Simple Performance Measures

Before proceeding to a more complex situation of studying accuracy of confidence intervals for estimated overfitting-corrected calibration curves, let's study the performance of ABCLOC confidence limits for standard scalar predictive accuracy measures: Somers' $D_{xy}$ (equal to $2\times (c - \frac{1}{2})$ where $c$ is the concordance probability or area under the ROC curve), the calibration slope on the logit scale, and the [Brier score](https://en.wikipedia.org/wiki/Brier_score).  The R `rms` package `validate.lrm` function uses, among other methods, the Efron-Gong optimism bootstrap for debiasing the apparent performance indexes to obtain overfitting-corrected scores estimating likely future performance of the model for these three, and other indexes.

Define a function to simulate a matrix of `p` predictors for a sample of size $n$, and a sample of size $n$ of binary responses from that predictor matrix, using a true model where the only non-zero regression coefficient for the $p$ predictors is $\beta_{1} = 1$.

```{r}
require(Hmisc)
require(rms)   # Version 8.1-0 is required
require(ggplot2)
require(data.table)
options(prType='html')

simd <- function(n, p) {
  x <- matrix(rnorm(n * p), n, p)
  L <- x[, 1]
  y <- ifelse(runif(n) <= plogis(L), 1, 0)
  list(x=x, y=y)
}
```

In what follows let $n=200$ be the sample size and $p=15$ be the number of predictors.  This is a setting with major overfitting.

To check confidence interval coverage we need to know the true Brier score for our data generating model.  This can be well estimated by simulating a single sample of 200,000 subjects, and computing the Brier score for many models fitted on 200 subjects, averaging their Brier scores. 

```{r}
set.seed(1)
N    <- 200
np   <- 15
Nbig <- 200000
big  <- simd(Nbig, np)
big  <- data.frame(x = I(big$x), y=big$y)   # I keeps matrix x as a matrix
Y    <- big$y
nsim <- 5000
```

Simulate `r nsim` samples of size 200.  For each sample run 300 bootstrap resamples to compute approximate confidence limits.  Also compute the true performance measures for each sample, to average.

```{r}
# Define a function to generate and analyze one dataset
sim1 <- function(n, p, B=300) {
  .i. <<- .i. + 1
  cat(.i., '', file='/tmp/z', append=TRUE)
  d    <- simd(n, np)
  f    <- lrm(y ~ x, data=d, x=TRUE, y=TRUE)
  phat <- predict(f, newdata=big, type='fitted')
  
  trueB     <- mean((phat - Y) ^ 2)
  trueDxy   <- somers2(phat, Y)['Dxy']
  trueSlope <- lrm.fit(qlogis(phat), Y)$coef[2]
  # saveraw is passed to predab.re-sample in rms 8.1-0 to cause indexes
  # from individual bootstrap resamples to be saved in a global object .predab_raw.
  v <- validate(f, B=B, saveraw=TRUE)
  r <- .predab_raw.
  w <- which(names(r$orig) %in% c('Dxy', 'Slope', 'B'))
  r$orig   <- r$orig[w]
  r$btrain <- r$btrain[, w]
  r$btest  <- r$btest [, w]
  praw[[.i.]] <<- r
  v <- v[w, c('index.corrected', 'Lower', 'Upper')]
  v <- cbind(v, true=c(trueDxy, trueSlope, trueB))
  v
}
.i.  <- 0
praw <- NULL
set.seed(1)
if(file.exists('indexes.rds')) {
  s    <- readRDS('indexes.rds')
  praw <- readRDS('praw.rds')
  } else {
  s <- replicate(nsim, sim1(n=N, p=np))
  saveRDS(s,    'indexes.rds')
  saveRDS(praw, 'praw.rds')
}

# Compute means across repetitions  
rn <- function(x) round(x, 3)
rn(apply(s, 1:2, mean))
```

The average overfitting-corrected Brier score estimate closely tracks the average of all the true Brier scores. The optimism bootstrap overestimated the performance on the $D_{xy}$ and calibration slope scales.  The positive bias is 0.02 for $D_{xy}$ and 0.04 for the calibration slope.[It is known that in extremely overfitted models, the bootstrap will underestimate the amount of overfitting when compared with 100 repeats of 10-fold cross-validation.  But even in these situations, model performance estimated by the bootstrap will convey enough bad news.]{.aside}

Next examine tail non-coverage probabilities for a variety of ways of utilizing the training and test indexes over the 300 resamples drawn for each simulated dataset.  This is done separately for each of the 3 performance indexes.

```{r}
# Add true indexes to praw
raw <- praw
for(i in 1 : nsim) raw[[i]]$true <- s[, 'true', i]

# Function applied to each element w of raw, for bootstrap type and basis and
# index number idx
# Result is a pair of logical values left and right with TRUE indicating that
# the true index is outside that tail

h <- function(w, what=c('error', 'cltrue')) {
  what  <- match.arg(what)
  true  <- w$true [idx]
  train <- w$btrain[, idx]
  test  <- w$btest [, idx]
  opt   <- train - test
  mopt  <- mean(opt)
  theta <- w$orig[idx]
  x <- switch(basis,
              train = train,
              test  = test,
              opt   = opt,
              wtd   = 2    * train - test,
              wtd2  = 1.5  * train - test,
              wtd3  = train - 0.75 * test,
              wtd4  = train - 1.25 * test,
              wtd5  = train - 1.5 * test)
  
  if(type == 'shifted nonpar') {
    qu <- quantile(x, c(0.025, 0.975), na.rm=TRUE)
    r <- c(qu[1] - mopt, qu[2] - mopt)
    } else if(substr(type, 1, 2) == 'sd') {
    if(basis == 'v2') {
      v1 <- var(train)
      v2 <- var(train - test)
      s  <- sqrt(v1 + v2 - 0.5 * sqrt(v1 * v2))
      r <- c(theta - mopt - s * z, theta - mopt + s * z)
    }  else {   # dualSD is in Hmisc 5.2-4
    s <- dualSD(x, nmin=if(type == 'sd1') 100000 else 10)
    a <- if(type == 'sd2') 1:2 else 2:1
    r <- c(theta - mopt - s[a[1]] * z, theta - mopt + s[a[2]] * z)
    }
    }
  if(what == 'cltrue') return(c(left=r[1], right=r[2], true=true))
  r <- c(r[1] > true, r[2] < true)
  names(r) <- .q(left, right)
  r
  }

z  <- qnorm(0.975)

# validate used wtd4 sd2rev; check against that for Brier score
if(FALSE) {
  idx  <- 3
  type <- 'sd2rev'
  basis <- 'wtd4'
  for(i in c(1, nsim / 2, nsim)) {
    cat('i=', i, '\n')
    print(h(raw[[i]], 'cltrue'))
    print(s[3,,i])
  }
}

types <- c('shifted nonpar', 'sd1', 'sd2', 'sd2rev')
bases <- c('train', 'test', 'opt', 'wtd', 'wtd2', 'wtd3', 'wtd4', 'wtd5', 'v2')

D <- vector('list', 3)
for(idx in 1 :3) {
  idxname <- c('Dxy', 'Slope', 'B')[idx]
  cat('\n-------------------------------------------------------------------\nIndex: ',
      if(idxname == 'B') 'Brier' else idxname, '\n\n', sep='')

  # Analyze simulation results separately for each confidence interval construction method
  # For each simulation compute the components of confidence intervals
  # Then compute the per-simulation limits
  # Then compute for that simulation whether a non-coverage has occurred per tail

  d <- NULL
  for(type in types) {
    for(basis in bases) {
      if(type == 'shifted nonpar' && basis %nin% c('train', 'test')) next
      if(type != 'sd1'            && basis == 'v2') next
      w    <- sapply(raw, h)
      err  <- apply(w, 1, mean, na.rm=TRUE)
      dist <- sum(abs(err - 0.025))
      d <- rbind(d, data.frame(type, basis, left=rn(err[1]), right=rn(err[2]),
                               coverage=rn(1 - sum(err)), dist=rn(dist)))
    }
  }
  row.names(d) <- NULL
  print(d)
  D[[idx]] <- d
}
```

For each bootstrap confidence interval method and each performance index, compute the sum of two distances from the target tail probabilities of 0.025, and then sum these distances over the three indexes and see which method has the best overall confidence interval coverage accuracy.

```{r}
# Sum distances over the 3 indexes
di <- D[[1]]$dist + D[[2]]$dist + D[[3]]$dist
cbind(distance=di, rank=rank(di))
```

The winner (rank of 1) is `sd2rev wtd4` which computes standard deviations of the 300 bootstrap re-sampled values of training - 1.25 $\times$ test indexes, where training denotes apparent performance on the bootstrap sample and test denotes the bootstrap fit's performance on the original whole sample.  The simple SD of the 300 estimates of bias (optimism; `sd1* opt`) had overall ranks of 14, 16, and 15.  It stands to reason that the best quantity on which to compute SDs puts more weight on the test sample indexes than it does on the training sample, because there is less variation in performance of bootstrap fits on the original sample over simulated datasets.

`sd2rev` refers to using the "top SD" for the lower confidence limit and the "bottom SD" for the upper confidence limit.  The distance rank for using the ordinary SD was 5.5 and the rank when reversing the two directional SDs was 13.  The standard parametric bootstrap using approximate normality does not handle asymmetric sampling distributions, but bottom and top SDs do.  Better performance using two SDs indicates asymmetry of sampling distributions for the indexes.

Coverage for calibration slope and Brier score was estimated to be 0.955 and 0.947 for the 
best overall performing method.  It did not perform so well for $D_{xy}$, giving only 0.852 coverage. $D_{xy}$ has a somewhat degenerate distribution when computed on regression fits, which typically do not allow $D_{xy} < 0$, so normality is usually not a good fit for $D_{xy}$.

# Accuracy of Estimated Calibration Curves and Confidence Limits

Let's study how well the overall winning bootstrap confidence interval estimator for three regular performance indexes fares when computing confidence bands for a vector of parameters, bootstrap overfitting-corrected calibration curves.
Let's fix the grid of 50 equally spaced points for which the calibration curve is estimated, using the 0.02 and 0.98 quantiles of pooled distributions of predicted probabilities for 10 repeated model fits.  The simulations we are about to run are similar to those for the 3 statistical indexes, but now we have 50 indexes.

A histogram of the pooled distribution shows that the entire range of predicted probabilities is represented by the chosen logistic model and distribution of $x_1$.

```{r}
set.seed(1)
phat <- replicate(10, {
  d <- simd(N, np)
  f <- lrm(y ~ x, data=d)
  predict(f, type='fitted')
})
round(apply(phat, 2, quantile, c(0, 0.02, 0.98, 1)), 2)
hist(phat, nclass=50, main=NULL, xlab=expression(hat(P)))
# On average for n=200, request calibrated probabilities for 5th to 196th
# largest observed predicted risks, which is pushing things a bit
r <- quantile(phat, probs=c(0.02, 0.98))
cat('Limits on predicted probabilities for which calibration curve is estimated:',
    round(r, 3), '\n')
pp <- seq(r[1], r[2], length=50)
```

For a given logistic model fit `f` on dataset `d` compute the true calibration curve at `pp`.  The calibration curve is a set of points $(\hat{P}, P)$, where $\hat{P}$ are predicted probabilities and $P$ are true probabilities that $Y=1$ given $\hat{P}$.  The points are fitted using least squares in the logit (the true calibration is linear in the logits by design) to average over variation in $X_{2}, \ldots, X_{`r np`}$ that yields unnecessary variation in $\hat{P}$ that arises from giving the last $p - 1$ (noise) $X$'s a chance to get nonzero $\hat{\beta}$'s.  So the true calibration curve is an average over covariate distributions, conditioning only on the covariates through conditioning on $\hat{P}$.

```{r}
truecal <- function(f, d, pl=FALSE) {
  Lhat  <- predict(f)
  Ltrue <- d$x[, 1]
  phat  <- plogis(Lhat)
  int_slope <- lm.fit.qr.bare(Lhat, Ltrue)$coefficients
  ptrue <- plogis(Ltrue)
  if(pl) plot(phat, ptrue, col='gray80')
  plogis(int_slope[1] + int_slope[2] * qlogis(pp))
}

# Test the function
par(mfrow=c(3,3), mar=c(2,2,0,0))
set.seed(3)
g <- function() {
  d <- simd(N, np)
  f <- lrm(y ~ x, data=d)
  tcal <- truecal(f, d, pl=TRUE)
  lines(pp, tcal)
  abline(a=0, b=1, col='gray80')
  coef(f)
}
round(replicate(9, g()), 2)
```

The fact that the circles are sometimes far from the curves implies that an "average" true calibration curve is hiding some false variation in predicted risks due to variation in the extraneous predictors.  In other words, many different predicted values (depending on the noise predictors) can give rise to one true probability.  To have a gold standard to compare with bootstrap estimates we must take averages.

The default calibration estimator used in `calibrate()` is _loess_, using the R `lowess` function with outlier detection turned off (`iter=0`).

Define alternative parametric linear and quadratic (in the logit) estimators.

```{r}
qcal <- function(x, y, deg=2) {
  # Curtail predicted probabilities to [0.001, 0.999]
  x <- pmax(0.001, pmin(x, 0.999))
  L <- qlogis(x)
  f <- lrm.fit(if(deg == 2) cbind(L, L^2) else L, y, compstats=FALSE)
  k <- coef(f)
  pgrid <- seq(min(x), max(x), length=200)
  Lgrid <- qlogis(pgrid)
  cgrid <- k[1] + k[2] * Lgrid
  if(deg == 2) cgrid <- cgrid + k[3] * Lgrid^2
  cal   <- plogis(cgrid)
  list(x=pgrid, y=cal)
}
```

Test the linear and quadratic calibrations in a setting where the calibration curve is known to be the line of identity, and also include _loess_ estimates (red).  Linear logistic calibration uses a black line, and quadratic a blue line.

```{r}
par(mfrow=c(3, 3), mar=c(2, 2, 0, 0))
set.seed(1)
for(i in 1 : 9) {
  x <- runif(N)
  y <- ifelse(runif(N) <= x, 1, 0)
  w1 <- qcal(x, y, deg=1)
  plot(w1, type='l', xlim=c(0, 1), ylim=c(0, 1))
  w2 <- qcal(x, y, deg=2)
  lines(w2, col='blue')
  l <- lowess(x, y, iter=0)
  lines(l, col='red')
  abline(a=0, b=1, col='grey80')
}
```

Define a function that draws one sample, fits a binary logistic model, and computes bootstrap percentile confidence bands for the calibration curve at a regular sequence of predicted probabilities `pp`. Add the true calibration curve to the output, alongside $\min(a, b)$, where $a$ is number of observed predicted risks that are less than the current value of `pp` and $b$ is the number of $\hat{P}$ that are greater than the value of `pp`.  In addition, save the standard deviation of $\hat{P}$.  This is done for one calibration estimator.  A special case is `smoother='basic'` which stands for using the `validate` function to get bias-corrected intercept and slope estimates and to convert them to linear (in the logit) calibration curves, without confidence limits.  Finally, `smoother='apparent'` does not use bootstrapping and instead takes the $\hat{P}$ to (falsely) need no calibration, to gauge the accuracy of uncalibrated estimates.

```{r}
# Use the following if you want to count "close" observations
# pptab <- data.table(pp, low = pp - 0.05, hi = pp + 0.05)
pptab <- data.table(ppgrid = pp)
sim1 <- function(n, p, B=300, smoother='lowess', pl=FALSE) {
  .i. <<- .i. + 1
  cat(.i., '', file='/tmp/z', append=TRUE)
  d <- simd(n, p)
  f <- lrm(y ~ x, data=d, x=TRUE, y=TRUE)
  phat    <- predict(f, type='fitted')
  ptrue   <- truecal(f, d)
  if(is.character(smoother) && smoother != 'lowess') {
    switch(smoother,
           basic = {
            val       <- validate(f, B=B)[, 'index.corrected']
            int_slope <- val[c('Intercept', 'Slope')]
            estcal    <- plogis(int_slope[1] + int_slope[2] * qlogis(pp)) },
           apparent = {
             estcal   <- pp } )
    lower <- NA
    upper <- NA
    if(pl) {
      plot(pp, estcal, type='l', xlim=c(0,1), ylim=c(0,1))
      lines(pp, ptrue, col='blue')
    }
  } else {
    bootcal <- calibrate(f, B=B, predy=pp, smoother=smoother)
    estcal <- bootcal[, 'calibrated.corrected']
    lower  <- pp + bootcal[, 'Lower']
    upper  <- pp + bootcal[, 'Upper']
    if(pl) {
      plot(bootcal, ylim=c(0,1))
      lines(pp, ptrue,   col='blue')
    }
  }
  d      <- data.table(phat)
  lo <- d[pptab, on = .(phat <= ppgrid), .N, by=.EACHI][, N]
  hi <- d[pptab, on = .(phat >= ppgrid), .N, by=.EACHI][, N]
  # count  <- d[pptab, on = .(phat >= low, phat <= hi), .N, by = .EACHI][, N]
  invisible(cbind(pp, estcal, ptrue, lower, upper, count=pmin(lo, hi), sphat=sd(phat)))
}
```

Run 9 tests for the default _loess_ calibration estimator, plotting the confidence bands.  The blue curve is the true calibration curve.  In the upper left of each panel show the number of observations with low and high predicted probabilities.

```{r}
#| column: screen-inset
#| fig-height: 8
#| fig-width: 8
set.seed(9)
par(mfrow=c(3,3), mar=c(2, 2, 0, 0))
for(j in 1 : 9) sim1(N, np, pl=TRUE)
```

Run 1000 simulations using all three calibration curve estimators.  For each dataset simulated, record the number of observations with low/high predictions.  Also compute the more direct estimated bias-corrected linear calibration curve by using the `validate` function to debias the intercept and slope computed from bootstrapping that is separate from the `calibrate` run.


```{r}
sm <- list('lowess',
           function(x, y) qcal(x, y, deg=1),
           function(x, y) qcal(x, y, deg=2),
           'basic',
           'apparent')
set.seed(7)
if(file.exists('cal.rds')) R <- readRDS('cal.rds') else {
  R        <- vector('list', 5)
  names(R) <- c('loess', 'linear', 'quadratic', 'basic', 'apparent')
  for(est in 1 : 5) {
    cat('\nEstimator', est, '\n')
    .i. <- 0
    r <- replicate(1000, sim1(N, np, B=300, smoother=sm[[est]]))  # 44m
    R[[est]] <- r
  }
  saveRDS(R, 'cal.rds')
}
```

Restructure the results into a list of data tables.

```{r}
convert2dt <- function(r) {
  npp  <- length(pp)
  nsim <- dim(r)[3]
  vn   <- dimnames(r)[[2]]
  # Restructure array so that pp and simulation # move fastest
  r <- aperm(r, c(1, 3, 2))
  # Stack into a taller matrix
  r <- matrix(r, nrow = npp * nsim, ncol=length(vn))
  d <- data.table(pp  = rep(pp, times=nsim),
                  sim = rep(1:nsim, each=npp) )
  d[, (vn) := as.data.table(r)]
  d
}
d <- lapply(R, convert2dt)
```

## Accuracy of Bootstrap Calibration Estimates for $n=200$

Group the number of points within 0.05 of the target estimated risk $\hat{P}$ into intervals and compute pseudo-medians[The pseudo-median is the median of all possible pairwise means.  It is more robust than the mean and is virtually as efficient as the mean, unlike the sample median.]{.aside} of absolute calibration curve estimation error by calibration estimator, target $\hat{P}$, and group, where groups are intervals of $\hat{P}$ SDs or minimum tail frequencies.

```{r}
pacc <- function(d, stratby) {
  # Restructure to a single tall and thin data table
  w <- rbindlist(d, idcol='estimator')
  if(! missing(stratby)) {
    w[, let(z         = abs(estcal - ptrue),
            group     = eval(stratby) ) ]
    w[, prn(table(group))]
    wall <- copy(w)
    wall[, group := 'All']
    w <- rbind(w, wall)
    w <- w[, .(err = pMedian(z, na.rm=TRUE)), by=.(estimator, group, pp)]
    g <- ggplot(w, aes(x=pp, y=err, color=estimator)) +
      facet_wrap(~ group)
  } else{
    w[, let(z = abs(estcal - ptrue))]
    w <- w[, .(err = pMedian(z, na.rm=TRUE)), by=.(estimator, pp)]
    g <- ggplot(w, aes(x=pp, y=err, color=estimator))
  }
  g + geom_line() + 
    xlab(expression(hat(P))) + ylab('pMedian |Calibration Estimation Error|')
}
pacc(d, expression(cut2(sphat, c(.175, .2, .25)))) + labs(caption='By groups of SD(Phat), n=200')
pacc(d, expression(cut2(count, 50))) + labs(caption='By groups of min(tail count), n=200')
```

It is typically true that even in heavily overfitted models, predictions that are near the grand mean estimated risk are accurate.  This is reflected in the above plots which show the "no calibration" method of just trusting the apparent predicted risks to have the lowest expected error in the middle of the risk distribution.  But it is highly overoptimistic for outer risks.

Run the following if you want to see a tensor spline surface for $x = \hat{P}$, $y=$ SD of $\hat{P}$.

```{r eval=FALSE}
require(mgcv)
for(nam in names(d)) {
  w <- d[[nam]]
  f <- gam(abs(estcal - ptrue) ~ te(pp, sphat), data=w)
  plot(f, scheme=2, main=nam)
}
```


## Accuracy of Approximate Bootstrap Confidence Limits for Calibration, $n=200$

For each value of `pp` compute the proportion of simulations for which `lower > ptrue` and the proportion `upper < ptrue` (tail non-coverage proportions).  Also compute overall error proportions.

```{r}
plotit <- function(R, title) {
  left  <- R[, 'lower', ] > R[, 'ptrue', ]
  right <- R[, 'upper', ] < R[, 'ptrue', ]
  
  rmn <- function(x) round(mean(x, na.rm=TRUE), 3)
  cat('Overall non-coverage for', title, ':\tleft=', rmn(left),
      ' right=', rmn(right), '\n')

  left  <- apply(left , 1, mean, na.rm=TRUE)
  right <- apply(right, 1, mean, na.rm=TRUE)
  nc    <- c(left, right)
  phat  <- c(pp, pp)
  k     <- length(left)
  side  <- c(rep('left', k), rep('right', k))
  ggplot(mapping=aes(x=phat, y=nc, col=side)) + geom_line() + xlab(expression(hat(P))) + 
      ylab('Non-Coverage') + ylim(0, 0.1) + labs(caption=title)
  }
for(nam in .q(loess, linear, quadratic))
  print(plotit(R[[nam]], title=nam))
```

The desired non-coverage probability is 0.025, which is achieved for interior predicted probabilities.  The problem with coverage probabilities in the tails seems to be due to wild estimates (especially for _loess_) or extrapolations at more rare predicted probabilities, i.e., those closer to 0 or 1.  Linear logistic calibration worked better than the more flexible calibration estimators.  Note that the simulations were stacked in favor of linear calibration.

## Calibration for $n=800$ and $n=1500$

Try two larger sample sizes.

```{r}
set.seed(8)
Ns <- c(800, 1500)
for(N in Ns) {
  fi <- paste0('cal', N, '.rds')
  if(file.exists(fi)) Rn <- readRDS(fi) else {
    Rn        <- vector('list', 5)
    names(Rn) <- c('loess', 'linear', 'quadratic', 'basic', 'apparent')
    for(est in 1 : 5) {
      cat('\nEstimator', est, '\n')
      i <- 0
      r <- replicate(1000, sim1(N, np, B=300, smoother=sm[[est]]))  # 91m
      Rn[[est]] <- r
    }
    saveRDS(Rn, fi)
  }
  if(N ==  800) R800  <- Rn
  if(N == 1500) R1500 <- Rn
}
```

Again compute accuracies of bias-corrected calibration estimates.

```{r}
for(N in Ns) {
  Rn <- if(N == 800) R800 else R1500
  d <- lapply(Rn, convert2dt)
  cap <- paste0('By groups of SD(Phat), n=', N)
  g <- pacc(d, stratby=expression(cut2(sphat, g=4))) + labs(caption=cap)
  print(g)
  cap <- paste0('By groups of min(tail counts), n=', N)
  g <- pacc(d, stratby=expression(cut2(count, c(20, 50, 100)))) + labs(caption=cap)
  print(g)
}
```

Now display confidence interval non-coverage proportions.

```{r}
for(N in Ns) {
  Rn <- if(N == 800) R800 else R1500
  for(nam in .q(loess, linear, quadratic))
    print(plotit(Rn[[nam]], title=paste0(nam, ', N=', N)))
}
```

Especially for linear and quadratic calibration, increasing the sample size made coverage larger than 0.95 for linear and quadratic calibration.  Perhaps it's best to have this conservatism so that the method will not be anti-conservative for small sample sizes.

# Summary

I considered approximate confidence bands for the true model performance indexes and entire calibration curves as estimated by a bootstrap overfitting-corrected approach.  The dual standard deviation approximate bootstrap confidence interval ABCLOC balances tail non-coverage probabilities by allowing the bootstrap distribution of estimated biases to be asymmetric.
ABCLOC seems to provide accurate enough confidence limits for individual accuracy scores such as the Brier score, calibration slope, and to a lesser extent Somers' $D_{xy}$, and I have a similar expectation of good performance for other scaler indexes.  Considering vector measures such as whole calibration curves evaluated over a sequence of predicted risks, ABCLOC can be conservative.  ABCLOC intervals are useful, and are certainly better than having no uncertainty intervals.  They require no additional computation time.

I hope that others will refine ABCLOC, and run more diverse simulations.

The choice of calibration curve estimator also matters, and an important point demonstrated by simulation is the difficulty of estimating calibration (not to mention its uncertainty) at very low or high predicted probabilities.  The more flexible _loess_ method has worse confidence interval coverage, especially for predicted risks far from the mean.  When the true calibration curve is logit-linear, linear calibration estimates, fitted by running binary logistic regression with a sole predictor equal to the logit of predicted risk, are superior as expected.  Quadratic calibration is a good compromise, and the performance of restricted cubic spline estimators with 4 knots should also be explored.  An advantage of parametric calibration methods (linear, quadratic, spline, ...) is that just the parameters (3 for quadratic) of the estimated calibration curve can be saved for each bootstrap fit, and one does not need to track the individual calibration curve points (50 in number in simulations presented here) as must be done for _loess_.[At present, the approach of saving only the calibration parameters is only implemented in `validate()` functions for calibration intercept and slope.]{.aside}

# Computing Environment

```{r}
grateful::cite_packages(pkgs='Session', output='paragraph', out.dir='.',
    cite.tidyverse=FALSE, omit=c('grateful', 'ggplot2'))
```

The code was run on `r sessionInfo()$running` on a Macbook Pro M2 Max.

